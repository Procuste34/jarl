{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 500000 # total timesteps of the experiment\n",
    "learning_rate = 3e-4 # the learning rate of the optimizer\n",
    "num_envs = 1 # the number of parallel environments\n",
    "num_steps = 128 # the number of steps to run in each environment per policy rollout\n",
    "gamma = 0.99 # the discount factor gamma\n",
    "gae_lambda = 0.95 # the lambda for the general advantage estimation\n",
    "num_minibatches = 4 # the number of mini batches\n",
    "update_epochs = 4 # the K epochs to update the policy\n",
    "clip_coef = 0.2 # the surrogate clipping coefficient\n",
    "ent_coef = 0.01 # coefficient of the entropy\n",
    "vf_coef = 0.5 # coefficient of the value function\n",
    "max_grad_norm = 0.5 # the maximum norm for the gradient clipping\n",
    "seed = 1 # seed for reproducible benchmarks\n",
    "exp_name = 'PPO' # unique experiment name\n",
    "env_id= \"CartPole-v1\" # id of the environment\n",
    "capture_video = False # whether to save video of agent gameplay\n",
    "\n",
    "batch_size = num_envs * num_steps # size of the batch after one rollout\n",
    "minibatch_size = batch_size // num_minibatches # size of the mini batch\n",
    "num_updates = total_timesteps // batch_size # the number of learning cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def make_env(env_id: string, idx: int, capture_video: bool, run_name: string):\n",
    "    def thunk():\n",
    "        if capture_video:\n",
    "            env = gym.make(env_id, render_mode='rgb_array')\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{env_id}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, i, capture_video, exp_name) for i in range(num_envs)]\n",
    ") # AsyncVectorEnv is faster, but we cannot extract single environment from it\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "obs, _ = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "# Helper function to quickly declare linear layer with weight and bias initializers\n",
    "def linear_layer_init(features, std=np.sqrt(2), bias_const=0.0):\n",
    "    layer = nn.Dense(features=features, kernel_init=nn.initializers.orthogonal(std), bias_init=nn.initializers.constant(bias_const))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import Array\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    action_n: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return nn.Sequential([\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(self.action_n, std=0.01),\n",
    "        ])(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return nn.Sequential([\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(1, std=1.0),\n",
    "        ])(x)\n",
    "\n",
    "actor = Actor(action_n=envs.single_action_space.n) # For jit we need to declare prod outside of class\n",
    "critic = Critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 14:41:07.317529: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-06-21 14:41:07.317574: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 27983872 bytes free, 11712004096 bytes total.\n",
      "2023-06-21 14:41:07.317608: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:453] Possibly insufficient driver version: 530.41.3\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrandom\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Setting seed of the environment for reproduction\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m key \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mPRNGKey(seed)\n\u001b[1;32m      5\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(seed)\n\u001b[1;32m      7\u001b[0m key, actor_key, critic_key, action_key, permutation_key \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msplit(key, num\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/random.py:155\u001b[0m, in \u001b[0;36mPRNGKey\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(seed):\n\u001b[1;32m    153\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPRNGKey accepts a scalar seed, but was given an array of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mshape(seed)\u001b[39m}\u001b[39;00m\u001b[39m != (). Use jax.vmap for batching\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m key \u001b[39m=\u001b[39m prng\u001b[39m.\u001b[39;49mseed_with_impl(impl, seed)\n\u001b[1;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m _return_prng_keys(\u001b[39mTrue\u001b[39;00m, key)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/prng.py:369\u001b[0m, in \u001b[0;36mseed_with_impl\u001b[0;34m(impl, seed)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseed_with_impl\u001b[39m(impl: PRNGImpl, seed: Union[\u001b[39mint\u001b[39m, Array]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PRNGKeyArrayImpl:\n\u001b[0;32m--> 369\u001b[0m   \u001b[39mreturn\u001b[39;00m random_seed(seed, impl\u001b[39m=\u001b[39;49mimpl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/prng.py:650\u001b[0m, in \u001b[0;36mrandom_seed\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m   seeds_arr \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39masarray(seeds)\n\u001b[0;32m--> 650\u001b[0m \u001b[39mreturn\u001b[39;00m random_seed_p\u001b[39m.\u001b[39;49mbind(seeds_arr, impl\u001b[39m=\u001b[39;49mimpl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    384\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/core.py:790\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 790\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/prng.py:662\u001b[0m, in \u001b[0;36mrandom_seed_impl\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m@random_seed_p\u001b[39m\u001b[39m.\u001b[39mdef_impl\n\u001b[1;32m    661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandom_seed_impl\u001b[39m(seeds, \u001b[39m*\u001b[39m, impl):\n\u001b[0;32m--> 662\u001b[0m   base_arr \u001b[39m=\u001b[39m random_seed_impl_base(seeds, impl\u001b[39m=\u001b[39;49mimpl)\n\u001b[1;32m    663\u001b[0m   \u001b[39mreturn\u001b[39;00m PRNGKeyArrayImpl(impl, base_arr)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/prng.py:667\u001b[0m, in \u001b[0;36mrandom_seed_impl_base\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandom_seed_impl_base\u001b[39m(seeds, \u001b[39m*\u001b[39m, impl):\n\u001b[1;32m    666\u001b[0m   seed \u001b[39m=\u001b[39m iterated_vmap_unary(seeds\u001b[39m.\u001b[39mndim, impl\u001b[39m.\u001b[39mseed)\n\u001b[0;32m--> 667\u001b[0m   \u001b[39mreturn\u001b[39;00m seed(seeds)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/prng.py:896\u001b[0m, in \u001b[0;36mthreefry_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mthreefry_seed\u001b[39m(seed: typing\u001b[39m.\u001b[39mArray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mArray:\n\u001b[1;32m    885\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Create a single raw threefry PRNG key from an integer seed.\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \n\u001b[1;32m    887\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39m    first padding out with zeros).\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 896\u001b[0m   \u001b[39mreturn\u001b[39;00m _threefry_seed(seed)\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[39m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details."
     ]
    }
   ],
   "source": [
    "import jax.random as random\n",
    "\n",
    "# Setting seed of the environment for reproduction\n",
    "key = random.PRNGKey(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "key, actor_key, critic_key, action_key, permutation_key = random.split(key, num=5)\n",
    "\n",
    "# Initializing agent parameters\n",
    "actor_params = actor.init(actor_key, obs)\n",
    "critic_params = critic.init(critic_key, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "# Anneal learning rate over time\n",
    "def linear_schedule(count):\n",
    "    frac = 1.0 - (count // (num_minibatches * update_epochs)) / num_updates\n",
    "    return learning_rate * frac\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core import FrozenDict\n",
    "from flax.struct import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AgentParams:\n",
    "    actor_params: FrozenDict\n",
    "    critic_params: FrozenDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from typing import Callable\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import struct\n",
    "\n",
    "# Probably jitting isn't needed as this functions should be jitted already\n",
    "actor.apply = jit(actor.apply)\n",
    "critic.apply = jit(critic.apply)\n",
    "\n",
    "class AgentState(TrainState):\n",
    "    # Setting default values for agent functions to make TrainState work in jitted function\n",
    "    actor_fn: Callable = struct.field(pytree_node=False)\n",
    "    critic_fn: Callable = struct.field(pytree_node=False)\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        actor_params=actor_params,\n",
    "        critic_params=critic_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    # As we have separated actor and critic we don't use apply_fn\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        actor_params=agent_state.params.actor_params,\n",
    "        critic_params=agent_state.params.critic_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Storage:\n",
    "    obs: jnp.array\n",
    "    actions: jnp.array\n",
    "    logprobs: jnp.array\n",
    "    dones: jnp.array\n",
    "    values: jnp.array\n",
    "    advantages: jnp.array\n",
    "    returns: jnp.array\n",
    "    rewards: jnp.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "#import tensorflow_probability.substrates.jax.distributions as tfp\n",
    "import distrax\n",
    "\n",
    "@jit\n",
    "def get_action_and_value(agent_state: AgentState, next_obs: ndarray, next_done: ndarray, storage: Storage, step: int, key: random.PRNGKeyArray):\n",
    "    action_logits = agent_state.actor_fn(agent_state.params.actor_params, next_obs)\n",
    "    value = agent_state.critic_fn(agent_state.params.critic_params, next_obs)\n",
    "\n",
    "    # Sample discrete actions from Normal distribution\n",
    "    #probs = tfp.Categorical(action_logits)\n",
    "    probs = distrax.Categorical(action_logits)\n",
    "    key, subkey = random.split(key)\n",
    "    action = probs.sample(seed=subkey)\n",
    "    logprob = probs.log_prob(action)\n",
    "    storage = storage.replace(\n",
    "        obs=storage.obs.at[step].set(next_obs),\n",
    "        dones=storage.dones.at[step].set(next_done),\n",
    "        actions=storage.actions.at[step].set(action),\n",
    "        logprobs=storage.logprobs.at[step].set(logprob),\n",
    "        values=storage.values.at[step].set(value.squeeze()),\n",
    "    )\n",
    "    return storage, action, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_action_and_value2(agent_state: AgentState, params: AgentParams, obs: ndarray, action: ndarray):\n",
    "    action_logits = agent_state.actor_fn(params.actor_params, obs)\n",
    "    value = agent_state.critic_fn(params.critic_params, obs)\n",
    "\n",
    "    #probs = tfp.Categorical(action_logits)\n",
    "    probs = distrax.Categorical(action_logits)\n",
    "    return probs.log_prob(action), probs.entropy(), value.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from flax.metrics.tensorboard import SummaryWriter\n",
    "from jax import device_get\n",
    "\n",
    "def rollout(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray,\n",
    "        global_step: int,\n",
    "):\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        storage, action, key = get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n",
    "        next_obs, reward, terminated, truncated, infos = envs.step(device_get(action))\n",
    "        next_done = terminated | truncated\n",
    "        storage = storage.replace(rewards=storage.rewards.at[step].set(reward))\n",
    "\n",
    "        # Only print when at least 1 env is done\n",
    "        if \"final_info\" not in infos:\n",
    "            continue\n",
    "\n",
    "        for info in infos[\"final_info\"]:\n",
    "            # Skip the envs that are not done\n",
    "            if info is None:\n",
    "                continue\n",
    "\n",
    "    return next_obs, next_done, storage, key, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_gae(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage\n",
    "):\n",
    "    # Reset advantages values\n",
    "    storage = storage.replace(advantages=storage.advantages.at[:].set(0.0))\n",
    "    next_value = agent_state.critic_fn(agent_state.params.critic_params, next_obs).squeeze()\n",
    "    # Compute advantage using generalized advantage estimate\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - storage.dones[t + 1]\n",
    "            nextvalues = storage.values[t + 1]\n",
    "        delta = storage.rewards[t] + gamma * nextvalues * nextnonterminal - storage.values[t]\n",
    "        lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        storage = storage.replace(advantages=storage.advantages.at[t].set(lastgaelam))\n",
    "    # Save returns as advantages + values\n",
    "    storage = storage.replace(returns=storage.advantages + storage.values)\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lax import stop_gradient\n",
    "\n",
    "@jit\n",
    "def ppo_loss(\n",
    "        agent_state: AgentState,\n",
    "        params: AgentParams,\n",
    "        obs: ndarray,\n",
    "        act: ndarray,\n",
    "        logp: ndarray,\n",
    "        adv: ndarray,\n",
    "        ret: ndarray,\n",
    "        val: ndarray,\n",
    "):\n",
    "    newlogprob, entropy, newvalue = get_action_and_value2(agent_state, params, obs, act)\n",
    "    logratio = newlogprob - logp\n",
    "    ratio = jnp.exp(logratio)\n",
    "\n",
    "    # Calculate how much policy is changing\n",
    "    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "    # Advantage normalization\n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    # Policy loss\n",
    "    pg_loss1 = -adv * ratio\n",
    "    pg_loss2 = -adv * jnp.clip(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "    pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "    # Value loss\n",
    "    v_loss_unclipped = (newvalue - ret) ** 2\n",
    "    v_clipped = val + jnp.clip(\n",
    "        newvalue - val,\n",
    "        -clip_coef,\n",
    "        clip_coef,\n",
    "    )\n",
    "    v_loss_clipped = (v_clipped - ret) ** 2\n",
    "    v_loss_max = jnp.maximum(v_loss_unclipped, v_loss_clipped)\n",
    "    v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "    # Entropy loss\n",
    "    entropy_loss = entropy.mean()\n",
    "\n",
    "    # main loss as sum of each part loss\n",
    "    loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "    return loss, (pg_loss, v_loss, entropy_loss, stop_gradient(approx_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "\n",
    "def update_ppo(\n",
    "        agent_state: AgentState,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray\n",
    "):\n",
    "    # Flatten collected experiences\n",
    "    b_obs = storage.obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = storage.logprobs.reshape(-1)\n",
    "    b_actions = storage.actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = storage.advantages.reshape(-1)\n",
    "    b_returns = storage.returns.reshape(-1)\n",
    "    b_values = storage.values.reshape(-1)\n",
    "\n",
    "    # Create function that will return gradient of the specified function\n",
    "    ppo_loss_grad_fn = jit(value_and_grad(ppo_loss, argnums=1, has_aux=True))\n",
    "\n",
    "    for epoch in range(update_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        b_inds = random.permutation(subkey, batch_size, independent=True)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(\n",
    "                agent_state,\n",
    "                agent_state.params,\n",
    "                b_obs[mb_inds],\n",
    "                b_actions[mb_inds],\n",
    "                b_logprobs[mb_inds],\n",
    "                b_advantages[mb_inds],\n",
    "                b_returns[mb_inds],\n",
    "                b_values[mb_inds],\n",
    "            )\n",
    "            # Update an agent\n",
    "            agent_state = agent_state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Calculate how good an approximation of the return is the value function\n",
    "    y_pred, y_true = b_values, b_returns\n",
    "    var_y = jnp.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "    return agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b64d9cb9c44fcbb21179a8a917f2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import signal\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Make kernel interrupt be handled as normal python error\n",
    "signal.signal(signal.SIGINT, signal.default_int_handler)\n",
    "\n",
    "run_name = f\"{exp_name}_{seed}_{time.asctime(time.localtime(time.time())).replace('  ', ' ').replace(' ', '_')}\"\n",
    "\n",
    "# Initialize the storage\n",
    "storage = Storage(\n",
    "    obs=jnp.zeros((num_steps, num_envs) + envs.single_observation_space.shape),\n",
    "    actions=jnp.zeros((num_steps, num_envs) + envs.single_action_space.shape),\n",
    "    logprobs=jnp.zeros((num_steps, num_envs)),\n",
    "    dones=jnp.zeros((num_steps, num_envs)),\n",
    "    values=jnp.zeros((num_steps, num_envs)),\n",
    "    advantages=jnp.zeros((num_steps, num_envs)),\n",
    "    returns=jnp.zeros((num_steps, num_envs)),\n",
    "    rewards=jnp.zeros((num_steps, num_envs)),\n",
    ")\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed)\n",
    "next_done = jnp.zeros(num_envs)\n",
    "\n",
    "try:\n",
    "    for update in tqdm(range(1, num_updates + 1)):\n",
    "        next_obs, next_done, storage, action_key, global_step = rollout(agent_state, next_obs, next_done, storage, action_key, global_step)\n",
    "        storage = compute_gae(agent_state, next_obs, next_done, storage)\n",
    "        agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, permutation_key = update_ppo(agent_state, storage, permutation_key)\n",
    "\n",
    "        if update%10==0:\n",
    "            avg_episodic_return = np.mean(device_get(storage.returns))\n",
    "            print(f\"update={update}, avg_episodic_return={avg_episodic_return}\")\n",
    "\n",
    "    print('Training complete!')\n",
    "except KeyboardInterrupt:\n",
    "    print('Training interrupted!')\n",
    "finally:\n",
    "    envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
