{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 500000 # total timesteps of the experiment\n",
    "learning_rate = 3e-4 # the learning rate of the optimizer\n",
    "num_envs = 1 # the number of parallel environments\n",
    "num_steps = 128 # the number of steps to run in each environment per policy rollout\n",
    "gamma = 0.99 # the discount factor gamma\n",
    "gae_lambda = 0.95 # the lambda for the general advantage estimation\n",
    "num_minibatches = 4 # the number of mini batches\n",
    "update_epochs = 4 # the K epochs to update the policy\n",
    "clip_coef = 0.2 # the surrogate clipping coefficient\n",
    "ent_coef = 0.01 # coefficient of the entropy\n",
    "vf_coef = 0.5 # coefficient of the value function\n",
    "max_grad_norm = 0.5 # the maximum norm for the gradient clipping\n",
    "seed = 1 # seed for reproducible benchmarks\n",
    "exp_name = 'PPO' # unique experiment name\n",
    "env_id= \"CartPole-v1\" # id of the environment\n",
    "capture_video = False # whether to save video of agent gameplay\n",
    "\n",
    "batch_size = num_envs * num_steps # size of the batch after one rollout\n",
    "minibatch_size = batch_size // num_minibatches # size of the mini batch\n",
    "num_updates = total_timesteps // batch_size # the number of learning cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def make_env(env_id: string, idx: int, capture_video: bool, run_name: string):\n",
    "    def thunk():\n",
    "        if capture_video:\n",
    "            env = gym.make(env_id, render_mode='rgb_array')\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{env_id}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, i, capture_video, exp_name) for i in range(num_envs)]\n",
    ") # AsyncVectorEnv is faster, but we cannot extract single environment from it\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "obs, _ = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "# Helper function to quickly declare linear layer with weight and bias initializers\n",
    "def linear_layer_init(features, std=np.sqrt(2), bias_const=0.0):\n",
    "    layer = nn.Dense(features=features, kernel_init=nn.initializers.orthogonal(std), bias_init=nn.initializers.constant(bias_const))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import Array\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    action_n: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return nn.Sequential([\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(self.action_n, std=0.01),\n",
    "        ])(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        return nn.Sequential([\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(64),\n",
    "            nn.tanh,\n",
    "            linear_layer_init(1, std=1.0),\n",
    "        ])(x)\n",
    "\n",
    "actor = Actor(action_n=envs.single_action_space.n) # For jit we need to declare prod outside of class\n",
    "critic = Critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as random\n",
    "\n",
    "# Setting seed of the environment for reproduction\n",
    "key = random.PRNGKey(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "key, actor_key, critic_key, action_key, permutation_key = random.split(key, num=5)\n",
    "\n",
    "# Initializing agent parameters\n",
    "actor_params = actor.init(actor_key, obs)\n",
    "critic_params = critic.init(critic_key, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "# Anneal learning rate over time\n",
    "def linear_schedule(count):\n",
    "    frac = 1.0 - (count // (num_minibatches * update_epochs)) / num_updates\n",
    "    return learning_rate * frac\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core import FrozenDict\n",
    "from flax.struct import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AgentParams:\n",
    "    actor_params: FrozenDict\n",
    "    critic_params: FrozenDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from typing import Callable\n",
    "from flax.training.train_state import TrainState\n",
    "from flax import struct\n",
    "\n",
    "# Probably jitting isn't needed as this functions should be jitted already\n",
    "actor.apply = jit(actor.apply)\n",
    "critic.apply = jit(critic.apply)\n",
    "\n",
    "class AgentState(TrainState):\n",
    "    # Setting default values for agent functions to make TrainState work in jitted function\n",
    "    actor_fn: Callable = struct.field(pytree_node=False)\n",
    "    critic_fn: Callable = struct.field(pytree_node=False)\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        actor_params=actor_params,\n",
    "        critic_params=critic_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    # As we have separated actor and critic we don't use apply_fn\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.inject_hyperparams(optax.adamw)(\n",
    "        learning_rate=linear_schedule,\n",
    "        eps=1e-5\n",
    "    )\n",
    ")\n",
    "\n",
    "agent_state = AgentState.create(\n",
    "    params=AgentParams(\n",
    "        actor_params=agent_state.params.actor_params,\n",
    "        critic_params=agent_state.params.critic_params\n",
    "    ),\n",
    "    tx=tx,\n",
    "    apply_fn=None,\n",
    "    actor_fn=actor.apply,\n",
    "    critic_fn=critic.apply\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Storage:\n",
    "    obs: jnp.array\n",
    "    actions: jnp.array\n",
    "    logprobs: jnp.array\n",
    "    dones: jnp.array\n",
    "    values: jnp.array\n",
    "    advantages: jnp.array\n",
    "    returns: jnp.array\n",
    "    rewards: jnp.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "#import tensorflow_probability.substrates.jax.distributions as tfp\n",
    "import distrax\n",
    "\n",
    "@jit\n",
    "def get_action_and_value(agent_state: AgentState, next_obs: ndarray, next_done: ndarray, storage: Storage, step: int, key: random.PRNGKeyArray):\n",
    "    action_logits = agent_state.actor_fn(agent_state.params.actor_params, next_obs)\n",
    "    value = agent_state.critic_fn(agent_state.params.critic_params, next_obs)\n",
    "\n",
    "    # Sample discrete actions from Normal distribution\n",
    "    #probs = tfp.Categorical(action_logits)\n",
    "    probs = distrax.Categorical(action_logits)\n",
    "    key, subkey = random.split(key)\n",
    "    action = probs.sample(seed=subkey)\n",
    "    logprob = probs.log_prob(action)\n",
    "    storage = storage.replace(\n",
    "        obs=storage.obs.at[step].set(next_obs),\n",
    "        dones=storage.dones.at[step].set(next_done),\n",
    "        actions=storage.actions.at[step].set(action),\n",
    "        logprobs=storage.logprobs.at[step].set(logprob),\n",
    "        values=storage.values.at[step].set(value.squeeze()),\n",
    "    )\n",
    "    return storage, action, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_action_and_value2(agent_state: AgentState, params: AgentParams, obs: ndarray, action: ndarray):\n",
    "    action_logits = agent_state.actor_fn(params.actor_params, obs)\n",
    "    value = agent_state.critic_fn(params.critic_params, obs)\n",
    "\n",
    "    #probs = tfp.Categorical(action_logits)\n",
    "    probs = distrax.Categorical(action_logits)\n",
    "    return probs.log_prob(action), probs.entropy(), value.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from flax.metrics.tensorboard import SummaryWriter\n",
    "from jax import device_get\n",
    "\n",
    "def rollout(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray,\n",
    "        global_step: int,\n",
    "):\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        storage, action, key = get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n",
    "        next_obs, reward, terminated, truncated, infos = envs.step(device_get(action))\n",
    "        next_done = terminated | truncated\n",
    "        storage = storage.replace(rewards=storage.rewards.at[step].set(reward))\n",
    "\n",
    "        # Only print when at least 1 env is done\n",
    "        if \"final_info\" not in infos:\n",
    "            continue\n",
    "\n",
    "        for info in infos[\"final_info\"]:\n",
    "            # Skip the envs that are not done\n",
    "            if info is None:\n",
    "                continue\n",
    "\n",
    "    return next_obs, next_done, storage, key, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_gae(\n",
    "        agent_state: AgentState,\n",
    "        next_obs: ndarray,\n",
    "        next_done: ndarray,\n",
    "        storage: Storage\n",
    "):\n",
    "    # Reset advantages values\n",
    "    storage = storage.replace(advantages=storage.advantages.at[:].set(0.0))\n",
    "    next_value = agent_state.critic_fn(agent_state.params.critic_params, next_obs).squeeze()\n",
    "    # Compute advantage using generalized advantage estimate\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - storage.dones[t + 1]\n",
    "            nextvalues = storage.values[t + 1]\n",
    "        delta = storage.rewards[t] + gamma * nextvalues * nextnonterminal - storage.values[t]\n",
    "        lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        storage = storage.replace(advantages=storage.advantages.at[t].set(lastgaelam))\n",
    "    # Save returns as advantages + values\n",
    "    storage = storage.replace(returns=storage.advantages + storage.values)\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lax import stop_gradient\n",
    "\n",
    "@jit\n",
    "def ppo_loss(\n",
    "        agent_state: AgentState,\n",
    "        params: AgentParams,\n",
    "        obs: ndarray,\n",
    "        act: ndarray,\n",
    "        logp: ndarray,\n",
    "        adv: ndarray,\n",
    "        ret: ndarray,\n",
    "        val: ndarray,\n",
    "):\n",
    "    newlogprob, entropy, newvalue = get_action_and_value2(agent_state, params, obs, act)\n",
    "    logratio = newlogprob - logp\n",
    "    ratio = jnp.exp(logratio)\n",
    "\n",
    "    # Calculate how much policy is changing\n",
    "    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "    # Advantage normalization\n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    # Policy loss\n",
    "    pg_loss1 = -adv * ratio\n",
    "    pg_loss2 = -adv * jnp.clip(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "    pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "    # Value loss\n",
    "    v_loss_unclipped = (newvalue - ret) ** 2\n",
    "    v_clipped = val + jnp.clip(\n",
    "        newvalue - val,\n",
    "        -clip_coef,\n",
    "        clip_coef,\n",
    "    )\n",
    "    v_loss_clipped = (v_clipped - ret) ** 2\n",
    "    v_loss_max = jnp.maximum(v_loss_unclipped, v_loss_clipped)\n",
    "    v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "    # Entropy loss\n",
    "    entropy_loss = entropy.mean()\n",
    "\n",
    "    # main loss as sum of each part loss\n",
    "    loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "    return loss, (pg_loss, v_loss, entropy_loss, stop_gradient(approx_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "\n",
    "def update_ppo(\n",
    "        agent_state: AgentState,\n",
    "        storage: Storage,\n",
    "        key: random.PRNGKeyArray\n",
    "):\n",
    "    # Flatten collected experiences\n",
    "    b_obs = storage.obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = storage.logprobs.reshape(-1)\n",
    "    b_actions = storage.actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = storage.advantages.reshape(-1)\n",
    "    b_returns = storage.returns.reshape(-1)\n",
    "    b_values = storage.values.reshape(-1)\n",
    "\n",
    "    # Create function that will return gradient of the specified function\n",
    "    ppo_loss_grad_fn = jit(value_and_grad(ppo_loss, argnums=1, has_aux=True))\n",
    "\n",
    "    for epoch in range(update_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        b_inds = random.permutation(subkey, batch_size, independent=True)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(\n",
    "                agent_state,\n",
    "                agent_state.params,\n",
    "                b_obs[mb_inds],\n",
    "                b_actions[mb_inds],\n",
    "                b_logprobs[mb_inds],\n",
    "                b_advantages[mb_inds],\n",
    "                b_returns[mb_inds],\n",
    "                b_values[mb_inds],\n",
    "            )\n",
    "            # Update an agent\n",
    "            agent_state = agent_state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Calculate how good an approximation of the return is the value function\n",
    "    y_pred, y_true = b_values, b_returns\n",
    "    var_y = jnp.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "    return agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3c6c27ac6840af98fd27caea0f90eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update=10, time=18.938044786453247, avg_episodic_return=12.114503860473633\n",
      "update=20, time=29.29999852180481, avg_episodic_return=12.802658081054688\n",
      "update=30, time=39.37170338630676, avg_episodic_return=17.867088317871094\n",
      "update=40, time=49.43215203285217, avg_episodic_return=20.589000701904297\n",
      "update=50, time=59.76343250274658, avg_episodic_return=21.328617095947266\n",
      "update=60, time=70.08083987236023, avg_episodic_return=26.392887115478516\n",
      "update=70, time=80.28735542297363, avg_episodic_return=28.683135986328125\n",
      "update=80, time=90.52149415016174, avg_episodic_return=33.98992156982422\n",
      "update=90, time=100.6041259765625, avg_episodic_return=33.05992889404297\n",
      "update=100, time=111.12665510177612, avg_episodic_return=34.42100143432617\n",
      "update=110, time=121.96621465682983, avg_episodic_return=35.09825134277344\n",
      "update=120, time=132.29921102523804, avg_episodic_return=36.207252502441406\n",
      "update=130, time=142.9062521457672, avg_episodic_return=41.90247344970703\n",
      "update=140, time=153.38334560394287, avg_episodic_return=41.6335563659668\n",
      "update=150, time=169.51510882377625, avg_episodic_return=49.788883209228516\n",
      "update=160, time=180.284423828125, avg_episodic_return=51.728946685791016\n",
      "update=170, time=191.30532312393188, avg_episodic_return=46.77142333984375\n",
      "Training interrupted!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import signal\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Make kernel interrupt be handled as normal python error\n",
    "signal.signal(signal.SIGINT, signal.default_int_handler)\n",
    "\n",
    "run_name = f\"{exp_name}_{seed}_{time.asctime(time.localtime(time.time())).replace('  ', ' ').replace(' ', '_')}\"\n",
    "\n",
    "# Initialize the storage\n",
    "storage = Storage(\n",
    "    obs=jnp.zeros((num_steps, num_envs) + envs.single_observation_space.shape),\n",
    "    actions=jnp.zeros((num_steps, num_envs) + envs.single_action_space.shape),\n",
    "    logprobs=jnp.zeros((num_steps, num_envs)),\n",
    "    dones=jnp.zeros((num_steps, num_envs)),\n",
    "    values=jnp.zeros((num_steps, num_envs)),\n",
    "    advantages=jnp.zeros((num_steps, num_envs)),\n",
    "    returns=jnp.zeros((num_steps, num_envs)),\n",
    "    rewards=jnp.zeros((num_steps, num_envs)),\n",
    ")\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed)\n",
    "next_done = jnp.zeros(num_envs)\n",
    "\n",
    "try:\n",
    "    for update in tqdm(range(1, num_updates + 1)):\n",
    "        next_obs, next_done, storage, action_key, global_step = rollout(agent_state, next_obs, next_done, storage, action_key, global_step)\n",
    "        storage = compute_gae(agent_state, next_obs, next_done, storage)\n",
    "        agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, explained_var, permutation_key = update_ppo(agent_state, storage, permutation_key)\n",
    "\n",
    "        if update%10==0:\n",
    "            avg_episodic_return = np.mean(device_get(storage.returns))\n",
    "            print(f\"update={update}, time={time.time()-start_time}, avg_episodic_return={avg_episodic_return}\")\n",
    "\n",
    "    print('Training complete!')\n",
    "except KeyboardInterrupt:\n",
    "    print('Training interrupted!')\n",
    "finally:\n",
    "    envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util GPU autour de 20%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
