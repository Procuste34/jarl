{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "from typing import Sequence\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import distrax #probs. distribs. en jax\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "#from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedNetwork(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.))(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        actor_head = nn.Dense(2, kernel_init=orthogonal(0.01), bias_init=constant(0.))(x)\n",
    "        pi = distrax.Categorical(logits=actor_head)\n",
    "        \n",
    "        return pi\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        critic_head = nn.Dense(1, kernel_init=orthogonal(1.), bias_init=constant(0.))(x)\n",
    "\n",
    "        return jnp.squeeze(critic_head, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flax.struct.dataclass\n",
    "class AgentParams:\n",
    "    shared_network_params: flax.core.FrozenDict\n",
    "    actor_params: flax.core.FrozenDict\n",
    "    critic_params: flax.core.FrozenDict\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class Storage:\n",
    "    obs: jnp.array\n",
    "    actions: jnp.array\n",
    "    logprobs: jnp.array\n",
    "    dones: jnp.array\n",
    "    values: jnp.array\n",
    "    advantages: jnp.array\n",
    "    returns: jnp.array\n",
    "    rewards: jnp.array\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class EpisodeStatistics:\n",
    "    episode_returns: jnp.array\n",
    "    episode_lengths: jnp.array\n",
    "    returned_episode_returns: jnp.array\n",
    "    returned_episode_lengths: jnp.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"CartPole-v1\")\n",
    "#env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "def make_env():\n",
    "    def thunk():\n",
    "        return gym.make(\"CartPole-v1\")\n",
    "    return thunk\n",
    "\n",
    "env = gym.vector.SyncVectorEnv([make_env()])\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    act = env.action_space.sample()\n",
    "    obs, rew, done, _, _ = env.step(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    act = env.action_space.sample()\n",
    "    obs, rew, done, _, _ = env.step(act)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m info\n",
      "\u001b[0;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "key, sn_key, a_key, c_key = jax.random.split(key, 4)\n",
    "\n",
    "episode_stats = EpisodeStatistics(episode_returns=jnp.zeros((1,), dtype=jnp.float32),\n",
    "                                  episode_lengths=jnp.zeros((1,), dtype=jnp.int32),\n",
    "                                  returned_episode_returns=jnp.zeros((1,), dtype=jnp.float32),\n",
    "                                  returned_episode_lengths=jnp.zeros((1,), dtype=jnp.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step_env(episode_stats: EpisodeStatistics, action):\n",
    "    next_obs, rew, terminated, truncated, info = env.step(jax.device_get(action))\n",
    "\n",
    "    next_done = terminated | truncated\n",
    "\n",
    "    new_episode_return = episode_stats.episode_returns + rew\n",
    "    new_episode_length = episode_stats.episode_lengths + 1\n",
    "\n",
    "    episode_stats = episode_stats.replace(\n",
    "            episode_returns=(new_episode_return) * (1 - next_done) * (1 - truncated),\n",
    "            episode_lengths=(new_episode_length) * (1 - next_done) * (1 - truncated),\n",
    "            returned_episode_returns=jnp.where(next_done + truncated, new_episode_return, episode_stats.returned_episode_returns),\n",
    "            returned_episode_lengths=jnp.where(next_done + truncated, new_episode_length, episode_stats.returned_episode_lengths)\n",
    "    )\n",
    "    \n",
    "    return episode_stats, (next_obs, rew, next_done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2.5e-4\n",
    "num_minibatches = 4 # \n",
    "update_epochs = 4 # nombre de updates de la policy\n",
    "num_steps = 128 # nombre de steps récoltés par env entre chaque update\n",
    "num_envs = 1\n",
    "batch_size = int(num_envs * num_steps) # nombre de steps total pour une update\n",
    "minibatch_size = int(batch_size // num_minibatches)\n",
    "total_timesteps = 1000000 # nombre total de steps du training\n",
    "num_updates = total_timesteps // batch_size # nombre total d'update du training\n",
    "\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "def linear_schedule(count):\n",
    "    # anneal learning rate linearly after one training iteration which contains\n",
    "    # (args.num_minibatches * args.update_epochs) gradient updates\n",
    "    frac = 1.0 - (count // (num_minibatches * update_epochs)) / num_updates\n",
    "    return learning_rate * frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64)\n"
     ]
    }
   ],
   "source": [
    "network = SharedNetwork()\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "network_params = network.init(sn_key, np.array([env.observation_space.sample()]))\n",
    "print(network_params[\"params\"][\"Dense_0\"][\"kernel\"].shape) #convention (fan_in, fan_out) pour les W\n",
    "\n",
    "agent_state = TrainState.create(apply_fn=None,\n",
    "                                params=AgentParams(network_params,\n",
    "                                            actor.init(a_key, network.apply(network_params, np.array([env.observation_space.sample()]))),\n",
    "                                            critic.init(c_key, network.apply(network_params, np.array([env.observation_space.sample()])))),\n",
    "                                tx=optax.chain(optax.clip_by_global_norm(max_grad_norm), optax.inject_hyperparams(optax.adam)(learning_rate=linear_schedule)))\n",
    "\n",
    "network.apply = jax.jit(network.apply)\n",
    "actor.apply = jax.jit(actor.apply)\n",
    "critic.apply = jax.jit(critic.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(\n",
    "    obs = jnp.zeros((num_steps, num_envs) + env.observation_space.shape), # (num_steps, num_envs, obs shape)\n",
    "    actions = jnp.zeros((num_steps, num_envs) + env.action_space.shape, dtype=jnp.int32), # (num_steps, num_envs, number of actions)\n",
    "    logprobs = jnp.zeros((num_steps, num_envs)), # (num_steps, num_envs)\n",
    "    dones = jnp.zeros((num_steps, num_envs)),\n",
    "    values = jnp.zeros((num_steps, num_envs)),\n",
    "    advantages = jnp.zeros((num_steps, num_envs)),\n",
    "    returns = jnp.zeros((num_steps, num_envs)),\n",
    "    rewards = jnp.zeros((num_steps, num_envs)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_action_and_value(agent_state, next_obs, next_done, storage:Storage, step, key):\n",
    "    hidden = network.apply(agent_state.params.shared_network_params, next_obs)\n",
    "\n",
    "    pi = actor.apply(agent_state.params.actor_params, hidden)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    action = pi.sample(seed=subkey)\n",
    "    logprob = pi.log_prob(action)\n",
    "\n",
    "    value = critic.apply(agent_state.params.critic_params, hidden)\n",
    "\n",
    "    storage = storage.replace(obs=storage.obs.at[step].set(next_obs),#pb de shape ici : reshape obs en (1, 4) ou bien ne plus inclure le num_envs dans les shapes\n",
    "                              actions=storage.actions.at[step].set(action),\n",
    "                              logprobs=storage.logprobs.at[step].set(logprob),\n",
    "                              dones=storage.dones.at[step].set(next_done),\n",
    "                              values=storage.values.at[step].set(value))\n",
    "    \n",
    "    return storage, action, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_action_and_value2(params, x:np.ndarray, action:np.ndarray):\n",
    "    hidden = network.apply(params.shared_network_params, x)\n",
    "\n",
    "    pi = actor.apply(agent_state.params.actor_params, hidden)\n",
    "    logprob = pi.log_prob(action)\n",
    "    p_logprob = pi.prob(action) * logprob #peut etre remplacé par exp(logprob) * logprob\n",
    "    entropy = -p_logprob.sum(-1)\n",
    "\n",
    "    value = critic.apply(agent_state.params.critic_params, hidden)\n",
    "\n",
    "    return logprob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "\n",
    "@jax.jit\n",
    "def compute_gae(agent_state, next_obs: np.ndarray, next_done: np.ndarray, storage):\n",
    "    storage = storage.replace(advantages=storage.advantages.at[:].set(0.0))\n",
    "\n",
    "    next_value = critic.apply(agent_state.params.critic_params, network.apply(agent_state.params.shared_network_params, next_obs))\n",
    "\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - storage.dones[t + 1]\n",
    "            nextvalues = storage.values[t + 1]\n",
    "        delta = storage.rewards[t] + gamma * nextvalues * nextnonterminal - storage.values[t]\n",
    "        lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        storage = storage.replace(advantages=storage.advantages.at[t].set(lastgaelam))\n",
    "    storage = storage.replace(returns=storage.advantages + storage.values)\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_adv = True\n",
    "clip_coef = 0.1\n",
    "ent_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "\n",
    "@jax.jit\n",
    "def update_ppo(agent_state: TrainState, storage: Storage, key: jax.random.PRNGKey,):\n",
    "    b_obs = storage.obs.reshape((-1,) + env.observation_space.shape)\n",
    "    b_logprobs = storage.logprobs.reshape(-1)\n",
    "    b_actions = storage.actions.reshape((-1,) + env.action_space.shape)\n",
    "    b_advantages = storage.advantages.reshape(-1)\n",
    "    b_returns = storage.returns.reshape(-1)\n",
    "\n",
    "    def ppo_loss(params, x, a, logp, mb_advantages, mb_returns):\n",
    "        newlogprob, entropy, newvalue = get_action_and_value2(params, x, a)\n",
    "        logratio = newlogprob - logp\n",
    "        ratio = jnp.exp(logratio)\n",
    "        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "        if norm_adv:\n",
    "            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "        # Policy loss\n",
    "        pg_loss1 = -mb_advantages * ratio\n",
    "        pg_loss2 = -mb_advantages * jnp.clip(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "        pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "        # Value loss\n",
    "        v_loss = 0.5 * ((newvalue - mb_returns) ** 2).mean()\n",
    "\n",
    "        entropy_loss = entropy.mean()\n",
    "        loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "        return loss, (pg_loss, v_loss, entropy_loss, jax.lax.stop_gradient(approx_kl))\n",
    "\n",
    "    ppo_loss_grad_fn = jax.value_and_grad(ppo_loss, has_aux=True)\n",
    "    for _ in range(update_epochs):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        b_inds = jax.random.permutation(subkey, batch_size, independent=True)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(\n",
    "                agent_state.params,\n",
    "                b_obs[mb_inds],\n",
    "                b_actions[mb_inds],\n",
    "                b_logprobs[mb_inds],\n",
    "                b_advantages[mb_inds],\n",
    "                b_returns[mb_inds],\n",
    "            )\n",
    "            agent_state = agent_state.apply_gradients(grads=grads)\n",
    "    return agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = env.reset()\n",
    "next_done = np.zeros(num_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit #jit seulement pour envs. compatibles avec JAX, car sinon les envs. non JAX veulent une action \"concrete\" et non pas tracé\n",
    "def rollout(agent_state, episode_stats, next_obs, next_done, storage, key, global_step):\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        storage, action, key = get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        episode_stats, (next_obs, reward, next_done, _) = _step_env(episode_stats, action)\n",
    "        storage = storage.replace(rewards=storage.rewards.at[step].set(reward))\n",
    "    return agent_state, episode_stats, next_obs, next_done, storage, key, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=128, avg_episodic_return=41.0\n",
      "global_step=256, avg_episodic_return=58.0\n",
      "global_step=384, avg_episodic_return=28.0\n",
      "global_step=512, avg_episodic_return=22.0\n",
      "global_step=640, avg_episodic_return=18.0\n",
      "global_step=768, avg_episodic_return=11.0\n",
      "global_step=896, avg_episodic_return=12.0\n",
      "global_step=1024, avg_episodic_return=13.0\n",
      "global_step=1152, avg_episodic_return=19.0\n",
      "global_step=1280, avg_episodic_return=15.0\n",
      "global_step=1408, avg_episodic_return=23.0\n",
      "global_step=1536, avg_episodic_return=10.0\n",
      "global_step=1664, avg_episodic_return=60.0\n",
      "global_step=1792, avg_episodic_return=15.0\n",
      "global_step=1920, avg_episodic_return=26.0\n",
      "global_step=2048, avg_episodic_return=21.0\n",
      "global_step=2176, avg_episodic_return=18.0\n",
      "global_step=2304, avg_episodic_return=21.0\n",
      "global_step=2432, avg_episodic_return=12.0\n",
      "global_step=2560, avg_episodic_return=14.0\n",
      "global_step=2688, avg_episodic_return=14.0\n",
      "global_step=2816, avg_episodic_return=20.0\n",
      "global_step=2944, avg_episodic_return=12.0\n",
      "global_step=3072, avg_episodic_return=19.0\n",
      "global_step=3200, avg_episodic_return=12.0\n",
      "global_step=3328, avg_episodic_return=11.0\n",
      "global_step=3456, avg_episodic_return=27.0\n",
      "global_step=3584, avg_episodic_return=16.0\n",
      "global_step=3712, avg_episodic_return=10.0\n",
      "global_step=3840, avg_episodic_return=36.0\n",
      "global_step=3968, avg_episodic_return=13.0\n",
      "global_step=4096, avg_episodic_return=16.0\n",
      "global_step=4224, avg_episodic_return=44.0\n",
      "global_step=4352, avg_episodic_return=13.0\n",
      "global_step=4480, avg_episodic_return=14.0\n",
      "global_step=4608, avg_episodic_return=11.0\n",
      "global_step=4736, avg_episodic_return=13.0\n",
      "global_step=4864, avg_episodic_return=14.0\n",
      "global_step=4992, avg_episodic_return=16.0\n",
      "global_step=5120, avg_episodic_return=11.0\n",
      "global_step=5248, avg_episodic_return=25.0\n",
      "global_step=5376, avg_episodic_return=19.0\n",
      "global_step=5504, avg_episodic_return=12.0\n",
      "global_step=5632, avg_episodic_return=33.0\n",
      "global_step=5760, avg_episodic_return=11.0\n",
      "global_step=5888, avg_episodic_return=17.0\n",
      "global_step=6016, avg_episodic_return=30.0\n",
      "global_step=6144, avg_episodic_return=41.0\n",
      "global_step=6272, avg_episodic_return=17.0\n",
      "global_step=6400, avg_episodic_return=17.0\n",
      "global_step=6528, avg_episodic_return=13.0\n",
      "global_step=6656, avg_episodic_return=14.0\n",
      "global_step=6784, avg_episodic_return=27.0\n",
      "global_step=6912, avg_episodic_return=14.0\n",
      "global_step=7040, avg_episodic_return=22.0\n",
      "global_step=7168, avg_episodic_return=46.0\n",
      "global_step=7296, avg_episodic_return=22.0\n",
      "global_step=7424, avg_episodic_return=30.0\n",
      "global_step=7552, avg_episodic_return=24.0\n",
      "global_step=7680, avg_episodic_return=12.0\n",
      "global_step=7808, avg_episodic_return=27.0\n",
      "global_step=7936, avg_episodic_return=18.0\n",
      "global_step=8064, avg_episodic_return=15.0\n",
      "global_step=8192, avg_episodic_return=13.0\n",
      "global_step=8320, avg_episodic_return=11.0\n",
      "global_step=8448, avg_episodic_return=15.0\n",
      "global_step=8576, avg_episodic_return=43.0\n",
      "global_step=8704, avg_episodic_return=33.0\n",
      "global_step=8832, avg_episodic_return=11.0\n",
      "global_step=8960, avg_episodic_return=22.0\n",
      "global_step=9088, avg_episodic_return=29.0\n",
      "global_step=9216, avg_episodic_return=11.0\n",
      "global_step=9344, avg_episodic_return=12.0\n",
      "global_step=9472, avg_episodic_return=11.0\n",
      "global_step=9600, avg_episodic_return=24.0\n",
      "global_step=9728, avg_episodic_return=18.0\n",
      "global_step=9856, avg_episodic_return=41.0\n",
      "global_step=9984, avg_episodic_return=18.0\n",
      "global_step=10112, avg_episodic_return=11.0\n",
      "global_step=10240, avg_episodic_return=13.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m update \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_updates \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     update_time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m     agent_state, episode_stats, next_obs, next_done, storage, key, global_step \u001b[39m=\u001b[39m rollout(\n\u001b[1;32m      4\u001b[0m         agent_state, episode_stats, next_obs, next_done, storage, key, global_step\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     storage \u001b[39m=\u001b[39m compute_gae(agent_state, next_obs, next_done, storage)\n\u001b[1;32m      7\u001b[0m     agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, key \u001b[39m=\u001b[39m update_ppo(\n\u001b[1;32m      8\u001b[0m         agent_state,\n\u001b[1;32m      9\u001b[0m         storage,\n\u001b[1;32m     10\u001b[0m         key,\n\u001b[1;32m     11\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[65], line 5\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(agent_state, episode_stats, next_obs, next_done, storage, key, global_step)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_steps):\n\u001b[1;32m      4\u001b[0m     global_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m*\u001b[39m num_envs\n\u001b[0;32m----> 5\u001b[0m     storage, action, key \u001b[39m=\u001b[39m get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n\u001b[1;32m      7\u001b[0m     \u001b[39m# TRY NOT TO MODIFY: execute the game and log data.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     episode_stats, (next_obs, reward, next_done, _) \u001b[39m=\u001b[39m _step_env(episode_stats, action)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/flax/struct.py:134\u001b[0m, in \u001b[0;36mdataclass.<locals>.clz_from_iterable\u001b[0;34m(meta, data)\u001b[0m\n\u001b[1;32m    132\u001b[0m data_args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(data_fields, data))\n\u001b[1;32m    133\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(meta_args \u001b[39m+\u001b[39m data_args)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m data_clz(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, obs, actions, logprobs, dones, values, advantages, returns, rewards)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for update in range(1, num_updates + 1):\n",
    "    update_time_start = time.time()\n",
    "    agent_state, episode_stats, next_obs, next_done, storage, key, global_step = rollout(\n",
    "        agent_state, episode_stats, next_obs, next_done, storage, key, global_step\n",
    "    )\n",
    "    storage = compute_gae(agent_state, next_obs, next_done, storage)\n",
    "    agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, key = update_ppo(\n",
    "        agent_state,\n",
    "        storage,\n",
    "        key,\n",
    "    )\n",
    "    avg_episodic_return = np.mean(jax.device_get(episode_stats.returned_episode_returns))\n",
    "    print(f\"global_step={global_step}, avg_episodic_return={avg_episodic_return}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
